{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4af6240-3de6-4f3f-a76d-2781e200d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import math\n",
    "\n",
    "class WindowAttention(layers.Layer):\n",
    "    def __init__(self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n",
    "        self.attn_drop = layers.Dropout(dropout_rate)\n",
    "        self.proj = layers.Dense(dim)\n",
    "        self.proj_drop = layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "        B_, N, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        qkv = tf.transpose(tf.reshape(self.qkv(x), shape=[-1, N, 3, self.num_heads, C // self.num_heads]), perm=[2, 0, 3, 1, 4])\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        q = q * self.scale\n",
    "        attn = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2]))\n",
    "        attn = tf.nn.softmax(attn, axis=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = tf.transpose(tf.matmul(attn, v), perm=[0, 2, 1, 3])\n",
    "        x = tf.reshape(x, shape=[-1, N, C])\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "class SwinTransformerBlock(layers.Layer):\n",
    "    def __init__(self, dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., qkv_bias=True, dropout_rate=0.0):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.attn = WindowAttention(dim, window_size, num_heads, qkv_bias, dropout_rate)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(int(dim * mlp_ratio)),\n",
    "            layers.Activation('gelu'),\n",
    "            layers.Dense(dim),\n",
    "        ])\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_resolution = input_shape[1]\n",
    "        self.H = self.W = int(math.sqrt(self.input_resolution))\n",
    "\n",
    "    def call(self, x):\n",
    "        B, L, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = tf.reshape(x, shape=[-1, self.H, self.W, C])\n",
    "\n",
    "        # Cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            shifted_x = tf.roll(x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2])\n",
    "        else:\n",
    "            shifted_x = x\n",
    "\n",
    "        # Partition windows\n",
    "        x_windows = self.window_partition(shifted_x, self.window_size)\n",
    "        x_windows = tf.reshape(x_windows, shape=[-1, self.window_size * self.window_size, C])\n",
    "\n",
    "        # W-MSA/SW-MSA\n",
    "        attn_windows = self.attn(x_windows)\n",
    "\n",
    "        # Merge windows\n",
    "        attn_windows = tf.reshape(attn_windows, shape=[-1, self.window_size, self.window_size, C])\n",
    "        shifted_x = self.window_reverse(attn_windows, self.window_size, self.H, self.W)\n",
    "\n",
    "        # Reverse cyclic shift\n",
    "        if self.shift_size > 0:\n",
    "            x = tf.roll(shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2])\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        x = tf.reshape(x, shape=[-1, self.H * self.W, C])\n",
    "\n",
    "        # FFN\n",
    "        x = shortcut + x\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def window_partition(self, x, window_size):\n",
    "        B, H, W, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2], tf.shape(x)[3]\n",
    "        x = tf.reshape(x, shape=[B, H // window_size, window_size, W // window_size, window_size, C])\n",
    "        windows = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
    "        windows = tf.reshape(windows, shape=[-1, window_size, window_size, C])\n",
    "        return windows\n",
    "\n",
    "    def window_reverse(self, windows, window_size, H, W):\n",
    "        B = tf.shape(windows)[0] // (H * W // window_size // window_size)\n",
    "        x = tf.reshape(windows, shape=[B, H // window_size, W // window_size, window_size, window_size, -1])\n",
    "        x = tf.transpose(x, perm=[0, 1, 3, 2, 4, 5])\n",
    "        x = tf.reshape(x, shape=[B, H, W, -1])\n",
    "        return x\n",
    "\n",
    "class PatchMerging(layers.Layer):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.reduction = layers.Dense(2 * dim, use_bias=False)\n",
    "        self.norm = layers.LayerNormalization(epsilon=1e-5)\n",
    "\n",
    "    def call(self, x):\n",
    "        H, W = self.H, self.W\n",
    "        B, L, C = tf.shape(x)[0], tf.shape(x)[1], tf.shape(x)[2]\n",
    "        assert L == H * W, \"input feature has wrong size\"\n",
    "        x = tf.reshape(x, shape=[B, H, W, C])\n",
    "\n",
    "        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n",
    "        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n",
    "        x = tf.concat([x0, x1, x2, x3], axis=-1)  # B H/2 W/2 4*C\n",
    "        x = tf.reshape(x, shape=[B, -1, 4 * C])  # B H/2*W/2 4*C\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        _, self.H, self.W, _ = input_shape\n",
    "\n",
    "def build_swin_transformer(input_shape, num_classes, num_layers, num_heads, window_size, mlp_dim, dropout_rate=0.0):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(mlp_dim, kernel_size=4, strides=4, padding='same')(inputs)\n",
    "    x = layers.Reshape((-1, x.shape[-1]))(x)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        x = SwinTransformerBlock(dim=mlp_dim, \n",
    "                                 num_heads=num_heads, \n",
    "                                 window_size=window_size,\n",
    "                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n",
    "                                 mlp_ratio=4,\n",
    "                                 qkv_bias=True,\n",
    "                                 dropout_rate=dropout_rate)(x)\n",
    "\n",
    "    x = layers.LayerNormalization(epsilon=1e-5)(x)\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe19029-731a-4f3d-881e-d42e8d575067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 72s 45ms/step - loss: 1.9342 - accuracy: 0.2636 - val_loss: 1.7909 - val_accuracy: 0.3255\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.7478 - accuracy: 0.3489 - val_loss: 1.7574 - val_accuracy: 0.3367\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.6724 - accuracy: 0.3861 - val_loss: 1.6513 - val_accuracy: 0.3879\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.6004 - accuracy: 0.4132 - val_loss: 1.5910 - val_accuracy: 0.4193\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.5612 - accuracy: 0.4283 - val_loss: 1.5739 - val_accuracy: 0.4246\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 1.5236 - accuracy: 0.4478 - val_loss: 1.5148 - val_accuracy: 0.4482\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 1.4860 - accuracy: 0.4593 - val_loss: 1.5057 - val_accuracy: 0.4516\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 70s 44ms/step - loss: 1.4699 - accuracy: 0.4641 - val_loss: 1.4655 - val_accuracy: 0.4714\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 69s 44ms/step - loss: 1.4353 - accuracy: 0.4787 - val_loss: 1.3964 - val_accuracy: 0.4941\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 70s 45ms/step - loss: 1.4022 - accuracy: 0.4936 - val_loss: 1.4849 - val_accuracy: 0.4648\n",
      "313/313 [==============================] - 5s 15ms/step - loss: 1.4849 - accuracy: 0.4648\n",
      "Test accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "def prepare_data():\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "    y_train, y_test = tf.keras.utils.to_categorical(y_train, 10), tf.keras.utils.to_categorical(y_test, 10)\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def train_and_evaluate():\n",
    "    (x_train, y_train), (x_test, y_test) = prepare_data()\n",
    "    \n",
    "    model = build_swin_transformer(input_shape=(32, 32, 3), num_classes=10, num_layers=4, num_heads=2, window_size=4, mlp_dim=128)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))\n",
    "    \n",
    "    _, accuracy = model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {accuracy:.2f}\")\n",
    "\n",
    "train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4152eab9-6045-47a4-9139-d192669a9505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
